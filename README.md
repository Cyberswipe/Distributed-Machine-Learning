# Distributed-Machine-Learning
A mix of statistical Modeling, Classification and Deep Learning to evaluate performance of Distributed ML Platforms on the MNIST Dataset. 

# Abstract
Machine learning has become increasingly popular across a wide range of application areas because of the ubiquity of big data and big computing. In recent times, dynamic research into several distributed computing platforms has plunged the world towards revolutionizing data processing in distributed platforms. This project aims to investigate the distributed machine learning platforms' architectural design because it inexorably influences the platforms' speed, scalability, and availability. Introducing TensorFlow in this project as an illustration of a more sophisticated dataflow system, and Spark as a representative dataflow system provides plethora of investigative data to study the given behavior of the system. We examine the communication and control bottlenecks for these methods from a distributed systems viewpoint. We also perform an investigative analysis on fault tolerance and development simplicity of the given system. This project aims to compare the performance of these two systems namely Spark and TensorFlow using two fundamental machine learning tasks—logistic regression and an example of image classification using the MNIST dataset—to provide a quantitative assessment.
